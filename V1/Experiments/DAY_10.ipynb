{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5iRHdUp-41L",
        "outputId": "08f61d9b-78a9-475c-990a-27f1d1de1387"
      },
      "outputs": [],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aW0nTlg_AWP",
        "outputId": "cad85309-7d77-4569-c441-7fa18acedca3"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmi0pN3t_F0N",
        "outputId": "32bfb14d-df66-40d1-bb32-476c70e4d2a8"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4MIXUSt_KHM",
        "outputId": "93a3b3a1-e9e3-4092-b66b-e3b858b95600"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS0v0shZrRWW",
        "outputId": "8da854d4-4108-4ba6-8a42-b25efa20cda5"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6__0zkx_P7L",
        "outputId": "6a1e28cb-e3ac-4973-c84f-f0ee1caaf2c0"
      },
      "outputs": [],
      "source": [
        "import fitz # PyMuPDF\n",
        "import spacy\n",
        "import gradio as gr\n",
        "import re\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"Environment Ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piVZ9MRAC7S8"
      },
      "outputs": [],
      "source": [
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aWDPh9iwld37"
      },
      "outputs": [],
      "source": [
        "# Load Engilsh model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pdO-tGHK_VK3"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_file):\n",
        "  \"\"\"\n",
        "  PDF text reader\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    doc = fitz.open(pdf_file.name)\n",
        "\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "      text += page.get_text()\n",
        "\n",
        "    cleaned_text = \" \".join(text.split())\n",
        "\n",
        "    if not cleaned_text.strip():\n",
        "      return \"Error: No text found, either it's not in pdf form or it's scanned image\"\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "  except Exception as e:\n",
        "    return f\"Error occurred: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8HhZ49GH84jh"
      },
      "outputs": [],
      "source": [
        "def extract_contact_info(text):\n",
        "    \"\"\"\n",
        "    Find specific contact details using Regex patterns\n",
        "    \"\"\"\n",
        "\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "    emails = re.findall(email_pattern, text)\n",
        "\n",
        "    # phone_pattern = r'^[6-9]\\d{9}$'\n",
        "    phone_pattern = r'[6-9]\\d{9}'\n",
        "    phones = re.findall(phone_pattern, text)\n",
        "\n",
        "    linkedin = re.findall(r'linkedin\\.com/in/[\\w.-]+', text)\n",
        "\n",
        "    github = re.findall(r'github\\.com/[\\w.-]+', text)\n",
        "\n",
        "    return {\n",
        "        \"Emails\": emails[0] if emails else \"Email Not Found\",\n",
        "        \"Phones\": phones[0] if phones else \"Phone Not Found\",\n",
        "        \"LinkedIn\": linkedin[0] if linkedin else \"Link Not Found\",\n",
        "        \"Github\": github[0] if github else \"Link Not Found\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZW0EY2NUDL1p"
      },
      "outputs": [],
      "source": [
        "# def clean_resume_text(text):\n",
        "#     text = text.lower()\n",
        "\n",
        "#     # Remove URLs, Emails and Phone numbers\n",
        "#     text = re.sub(r'\\S+@\\S+','', text)\n",
        "#     text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "#     # Remove special characters and numbers (alphanumeric and basic punctuation)\n",
        "#     text = re.sub(r'[^a-zA-Z\\s]', ' ', str(text))\n",
        "\n",
        "#     # Tokenize and remove stopwords\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     words = nltk.word_tokenize(text)\n",
        "#     filtered_text = [w for w in words if w not in stop_words]\n",
        "\n",
        "#     return \" \".join(filtered_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Wqq1YrrYk0Wx"
      },
      "outputs": [],
      "source": [
        "def extract_entities(text):\n",
        "    \"\"\"\n",
        "    Identify Names and Organizations using spaCy\n",
        "    \"\"\"\n",
        "\n",
        "    doc = nlp(text)\n",
        "    entities = {\n",
        "        \"Name\": [],\n",
        "        \"Organizations\": []\n",
        "    }\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            entities[\"Name\"].append(ent.text)\n",
        "        elif ent.label_ == \"ORG\":\n",
        "            entities[\"Organizations\"].append(ent.text)\n",
        "\n",
        "    primary_name = entities[\"Name\"][0] if entities[\"Name\"] else \"Not Identified\"\n",
        "\n",
        "    return {\n",
        "        \"Candidate Name\": primary_name,\n",
        "        \"All Names Found\": list(set(entities[\"Name\"])),\n",
        "        \"Comapanies/Institutions\": list(set(entities[\"Organizations\"]))\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dXUkLQqtoauZ"
      },
      "outputs": [],
      "source": [
        "def extract_skills(text):\n",
        "    # nlp = spacy.load(\"en_core_web_sm\")\n",
        "    matcher = PhraseMatcher(nlp.vocab, attr = \"LOWER\")\n",
        "\n",
        "    # Skill categories\n",
        "    skills_db = {\n",
        "        \"Programming\": [\"Python\", \"Java\", \"C++\", \"JavaScript\", \"SQL\", \"GO\", \"Rust\"],\n",
        "        \"Machine Learning\": [\"PyTorch\", \"TensorFlow\", \"Scikit-learn\", \"NLP\", \"Computer Vision\"],\n",
        "        \"Cloud\": [\"AWS\", \"Azure\", \"Docker\", \"Kubernetes\", \"GCP\"],\n",
        "        \"Tools\": [\"Git\", \"Jira\", \"Excel\", \"Tableau\"]\n",
        "    }\n",
        "\n",
        "    # Add petterns to matcher\n",
        "    for category, skill_list in skills_db.items():\n",
        "        patterns = [nlp.make_doc(skill) for skill in skill_list]\n",
        "        matcher.add(category, patterns)\n",
        "\n",
        "    doc = nlp(text)\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    # Extraction of found skills\n",
        "    found_skills = {}\n",
        "    for match_id, start, end in matches:\n",
        "        category = nlp.vocab.strings[match_id]\n",
        "        skill = doc[start:end].text\n",
        "\n",
        "        if category not in found_skills:\n",
        "            found_skills[category] = set()\n",
        "\n",
        "        found_skills[category].add(skill)\n",
        "\n",
        "    # Conversion from sets to list for JSON output\n",
        "    return {k: list(v) for k, v in found_skills.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "a1DLtvLztjDP"
      },
      "outputs": [],
      "source": [
        "# def segment_resume(text):\n",
        "#     # Standardized headers\n",
        "#     sections = [\n",
        "#         'EDUCATION',\n",
        "#         'EXPERIENCE',\n",
        "#         'WORK EXPERIENCE',\n",
        "#         'PROJECTS',\n",
        "#         'SKILLS',\n",
        "#         'CERTIFICATIONS',\n",
        "#         'SUMMARY'\n",
        "#     ]\n",
        "\n",
        "#     header_pattern = r'(?i)\\b(?:'+'|'.join(sections) + r')\\b'\n",
        "\n",
        "#     # Find all matches and their positions\n",
        "#     matches = list(re.finditer(header_pattern, text))\n",
        "\n",
        "#     segmented_data = {}\n",
        "\n",
        "#     if not matches:\n",
        "#         return {\"Full text\":text}\n",
        "\n",
        "#     # Iterate through matches to slice the text\n",
        "#     for i in range(len(matches)):\n",
        "#         start_idx = matches[i].start()\n",
        "#         header_name = matches[i].group().upper()\n",
        "\n",
        "#         # The end of this section is start of the next one\n",
        "#         if i + 1 < len(matches):\n",
        "#             end_idx = matches[i+1].start()\n",
        "#         else:\n",
        "#             end_idx = len(text)\n",
        "\n",
        "#         content = text[start_idx:end_idx].replace(header_name, \"\").strip()\n",
        "#         segmented_data[header_name] = content\n",
        "\n",
        "#     return segmented_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4q9oLdEWxPNP"
      },
      "outputs": [],
      "source": [
        "# def safe_resume_parser(pdf_file):\n",
        "#     try:\n",
        "#         # Extraction with validation\n",
        "#         raw_text = extract_text_from_pdf(pdf_file)\n",
        "#         if \"Error\" in raw_text:\n",
        "#             return {\"Status\":\"Failed\", \"Reason\":\"Invalid PDF or Image_based PDF\"}\n",
        "\n",
        "#         # Sequential Extraction\n",
        "#         contacts = extract_contact_info(raw_text)\n",
        "#         entities = extract_entities(raw_text)\n",
        "#         segments = segment_resume(raw_text)\n",
        "#         skills = extract_skills(raw_text)\n",
        "\n",
        "#         # Handle missing data\n",
        "#         candidate_name = entities.get(\"Candidate Name\", \"Not Identified\")\n",
        "#         if candidate_name == \"Not Identified\" and contacts[\"Emails\"] != \"Not Found\":\n",
        "#             candidate_name = contacts[\"Emails\"].split('@')[0].capitalize()\n",
        "\n",
        "#         return {\n",
        "#             \"Status\": \"Success\",\n",
        "#             \"Metadata\": {\n",
        "#                 \"Filename\": pdf_file.name.split('/')[-1],\n",
        "#                 \"Text_Length\": len(raw_text)\n",
        "#             },\n",
        "#             \"Extracted_Data\": {\n",
        "#                 \"Name\": candidate_name,\n",
        "#                 \"Contact\": contacts,\n",
        "#                 \"Sections_Detected\": list(segments.keys()),\n",
        "#                 \"Skills\": skills\n",
        "#             }\n",
        "#         }\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return {\"Status\": \"Critical Error\", \"Details\": str(e)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fPhZFj3vqhrN"
      },
      "outputs": [],
      "source": [
        "def calculate_match_score(resume_text, job_description):\n",
        "    \"\"\"\n",
        "    Similarity score between a resume and a JD\n",
        "    \"\"\"\n",
        "\n",
        "    text_list = [resume_text, job_description]\n",
        "\n",
        "    # Initialize Vectorizer\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    # Transform text into a matrix of numbers(vectors)\n",
        "    tfidf_matrix = vectorizer.fit_transform(text_list)\n",
        "\n",
        "    # Calculate similarity\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "\n",
        "    # Score as a percentage\n",
        "    match_score = round(similarity_matrix[0][0] * 100, 2)\n",
        "\n",
        "    return match_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3OSNcpXevvCb"
      },
      "outputs": [],
      "source": [
        "def  analyze_skill_gap(resume_skills_dict, jd_text):\n",
        "    \"\"\"\n",
        "    Identifies which required skills are missing from the resume.\n",
        "    \"\"\"\n",
        "\n",
        "    jd_skills_dict = extract_skills(jd_text)\n",
        "\n",
        "    # Flatten both dictionaries into simple lists/sets of skills\n",
        "    resume_skills_set = set([skill.lower() for sublist in resume_skills_dict.values() for skill in sublist])\n",
        "    jd_skills_set = set([skill.lower() for sublist in jd_skills_dict.values() for skill in sublist])\n",
        "\n",
        "    matched = jd_skills_set.intersection(resume_skills_set)\n",
        "    missing = jd_skills_set.difference(resume_skills_set)\n",
        "\n",
        "    return {\n",
        "        \"Matched_Skills\": list(matched),\n",
        "        \"Missing_Skills\": list(missing),\n",
        "        \"skill_coverage\": f\"{len(matched)} / {len(jd_skills_set)}\" if jd_skills_set else \"N/A\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GCE8pxhtVrYV"
      },
      "outputs": [],
      "source": [
        "def final_resume_analyzer(pdf_file, job_description):\n",
        "    # Pipeline Execution\n",
        "    raw_text = extract_text_from_pdf(pdf_file)\n",
        "    contacts = extract_contact_info(raw_text)\n",
        "    entities = extract_entities(raw_text)\n",
        "    resume_skills = extract_skills(raw_text)\n",
        "\n",
        "    # Logic layer\n",
        "    score = calculate_match_score(raw_text, job_description)\n",
        "    gap_analysis = analyze_skill_gap(resume_skills, job_description)\n",
        "\n",
        "    # Decision logic\n",
        "    status = \"Shortlist\" if score > 70 else \"Review\" if score > 40 else \"Reject\"\n",
        "\n",
        "    return {\n",
        "        \"Candidate Profile\": {\n",
        "            \"Name\": entities[\"Candidate Name\"],\n",
        "            \"Contact\" : contacts,\n",
        "            \"Top Skills\": resume_skills\n",
        "        },\n",
        "        \"ATS Analysis\": {\n",
        "            \"Match Score\": f\"{score}%\",\n",
        "            \"Recommendation\": status,\n",
        "            \"Missing Keywords\": gap_analysis[\"Missing_Skills\"]\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mUP17LHs9R0p"
      },
      "outputs": [],
      "source": [
        "# UI Update\n",
        "interface = gr.Interface(\n",
        "    fn = final_resume_analyzer,\n",
        "    inputs = [\n",
        "        gr.File(label = \"Upload Resume\"),\n",
        "        gr.Textbox(label = \"Job Description\", lines = 8)\n",
        "    ],\n",
        "    outputs = [\n",
        "        gr.JSON(label = \"Analysis\")\n",
        "    ],\n",
        "    title = \"AI Resume Parser\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "R74lHqvO_n4W",
        "outputId": "e528104c-2e00-4565-a425-04242e465991"
      },
      "outputs": [],
      "source": [
        "interface.launch(debug = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e32cb280"
      },
      "source": [
        "### Sample Job Description\n",
        "\n",
        "**Job Title:** Senior Machine Learning Engineer\n",
        "\n",
        "**Company:** Tech Innovations Inc.\n",
        "\n",
        "**Location:** Remote\n",
        "\n",
        "**About Us:**\n",
        "Tech Innovations Inc. is a leading technology company at the forefront of developing cutting-edge AI solutions. We are looking for a highly skilled and motivated Senior Machine Learning Engineer to join our dynamic team and contribute to the design, development, and deployment of advanced machine learning models.\n",
        "\n",
        "**Responsibilities:**\n",
        "*   Design, develop, and deploy scalable machine learning models into production.\n",
        "*   Collaborate with data scientists and product managers to understand business requirements and translate them into technical specifications.\n",
        "*   Optimize existing machine learning algorithms for performance and efficiency.\n",
        "*   Implement and maintain robust MLOps practices, including CI/CD pipelines, model monitoring, and version control.\n",
        "*   Conduct rigorous testing and validation of models to ensure accuracy and reliability.\n",
        "*   Stay up-to-date with the latest advancements in machine learning and artificial intelligence.\n",
        "*   Mentor junior engineers and contribute to a culture of continuous learning.\n",
        "\n",
        "**Required Qualifications:**\n",
        "*   Master's or Ph.D. in Computer Science, Machine Learning, Statistics, or a related field.\n",
        "*   5+ years of experience in machine learning engineering.\n",
        "*   Strong proficiency in Python and experience with libraries such as TensorFlow, PyTorch, or scikit-learn.\n",
        "*   Extensive experience with cloud platforms (AWS, GCP, Azure) and containerization technologies (Docker, Kubernetes).\n",
        "*   Solid understanding of data structures, algorithms, and software design principles.\n",
        "*   Experience with big data technologies (e.g., Spark, Hadoop).\n",
        "*   Excellent communication and teamwork skills.\n",
        "\n",
        "**Preferred Qualifications:**\n",
        "*   Experience with Natural Language Processing (NLP) or Computer Vision (CV).\n",
        "*   Familiarity with distributed machine learning frameworks.\n",
        "*   Publications in top-tier conferences or journals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17bb3660"
      },
      "source": [
        "```markdown\n",
        "# YOUR NAME\n",
        "\n",
        "[Email: your.email@example.com](mailto:your.email@example.com) | [Phone: (123) 456-7890](tel:1234567890) | [LinkedIn: linkedin.com/in/yourprofile](https://www.linkedin.com/in/yourprofile) | [GitHub: github.com/yourprofile](https://github.com/yourprofile)\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "A highly motivated and results-oriented professional with X years of experience in [Your Field]. Seeking to leverage expertise in [Key Skill 1], [Key Skill 2], and [Key Skill 3] to contribute to [Company Name]'s success as a [Job Title].\n",
        "\n",
        "---\n",
        "\n",
        "## Skills\n",
        "\n",
        "**Programming Languages:** Python, Java, C++, JavaScript, SQL\n",
        "\n",
        "**Machine Learning:** PyTorch, TensorFlow, Scikit-learn, NLP, Computer Vision, Data Analysis, Predictive Modeling\n",
        "\n",
        "**Cloud Platforms:** AWS, Azure, GCP, Docker, Kubernetes\n",
        "\n",
        "**Tools & Technologies:** Git, Jira, Tableau, Excel, Linux\n",
        "\n",
        "**Other:** Problem Solving, Communication, Teamwork, Project Management\n",
        "\n",
        "---\n",
        "\n",
        "## Experience\n",
        "\n",
        "### Senior Machine Learning Engineer | Tech Solutions Inc. | City, State\n",
        "\n",
        "**Month, Year – Present**\n",
        "\n",
        "*   Designed and implemented scalable machine learning models for [Specific Project/Product].\n",
        "*   Optimized existing algorithms, resulting in a 20% improvement in model performance and a 15% reduction in computational cost.\n",
        "*   Collaborated with cross-functional teams to define project requirements and deploy solutions.\n",
        "*   Developed and maintained CI/CD pipelines for automated model deployment and monitoring.\n",
        "\n",
        "### Machine Learning Engineer | Data Insights Co. | City, State\n",
        "\n",
        "**Month, Year – Month, Year**\n",
        "\n",
        "*   Built and evaluated machine learning models for [Specific Application].\n",
        "*   Conducted extensive data preprocessing and feature engineering.\n",
        "*   Contributed to the research and development of new AI technologies.\n",
        "\n",
        "---\n",
        "\n",
        "## Education\n",
        "\n",
        "### Master of Science in Computer Science | University Name | City, State\n",
        "\n",
        "**Month, Year – Month, Year**\n",
        "\n",
        "*   Specialization: Artificial Intelligence, Machine Learning\n",
        "*   Relevant Coursework: Advanced Machine Learning, Deep Learning, Natural Language Processing\n",
        "\n",
        "### Bachelor of Science in Electrical Engineering | University Name | City, State\n",
        "\n",
        "**Month, Year – Month, Year**\n",
        "\n",
        "---\n",
        "\n",
        "## Projects\n",
        "\n",
        "### Project Title 1\n",
        "\n",
        "*   Developed a [brief description of project] using [technologies used].\n",
        "*   Achieved [quantifiable result or impact].\n",
        "*   [Link to project/GitHub]\n",
        "\n",
        "### Project Title 2\n",
        "\n",
        "*   [Description]\n",
        "\n",
        "---\n",
        "\n",
        "## Certifications\n",
        "\n",
        "*   AWS Certified Machine Learning – Specialty\n",
        "*   Google Cloud Professional Machine Learning Engineer\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fea98efd"
      },
      "source": [
        "You can use this job description to create a resume that includes relevant skills (Python, TensorFlow, PyTorch, AWS, GCP, Azure, Docker, Kubernetes, NLP, Computer Vision, etc.) and experience. You can then upload this created resume PDF and paste the above job description into the Gradio interface to test the resume parser and analyzer."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
