{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5iRHdUp-41L",
        "outputId": "3cc56536-4bb5-471d-9f8a-8419740ec718"
      },
      "outputs": [],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aW0nTlg_AWP",
        "outputId": "73239278-b289-41fc-dc54-07dcfe781d1c"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmi0pN3t_F0N",
        "outputId": "c1dd1ce8-b113-48a6-f33b-310067d59556"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4MIXUSt_KHM",
        "outputId": "27dbdc4d-0183-4cb5-934d-cd6619f8eacf"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pS0v0shZrRWW",
        "outputId": "902146c1-efd0-4893-a49a-f64318e14c61"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6__0zkx_P7L",
        "outputId": "ca60c952-0528-4218-f3d0-83a814812606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment Ready!\n"
          ]
        }
      ],
      "source": [
        "import fitz # PyMuPDF\n",
        "import spacy\n",
        "import gradio as gr\n",
        "import re\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "print(\"Environment Ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piVZ9MRAC7S8"
      },
      "outputs": [],
      "source": [
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aWDPh9iwld37"
      },
      "outputs": [],
      "source": [
        "# Load Engilsh model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pdO-tGHK_VK3"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_file):\n",
        "  \"\"\"\n",
        "  PDF text reader\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    doc = fitz.open(pdf_file.name)\n",
        "\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "      text += page.get_text()\n",
        "\n",
        "    cleaned_text = \" \".join(text.split())\n",
        "\n",
        "    if not cleaned_text.strip():\n",
        "      return \"Error: No text found, either it's not in pdf form or it's scanned image\"\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "  except Exception as e:\n",
        "    return f\"Error occurred: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8HhZ49GH84jh"
      },
      "outputs": [],
      "source": [
        "def extract_contact_info(text):\n",
        "    \"\"\"\n",
        "    Find specific contact details using Regex patterns\n",
        "    \"\"\"\n",
        "\n",
        "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "    emails = re.findall(email_pattern, text)\n",
        "\n",
        "    # phone_pattern = r'^[6-9]\\d{9}$'\n",
        "    phone_pattern = r'[6-9]\\d{9}'\n",
        "    phones = re.findall(phone_pattern, text)\n",
        "\n",
        "    linkedin = re.findall(r'linkedin\\.com/in/[\\w.-]+', text)\n",
        "\n",
        "    github = re.findall(r'github\\.com/[\\w.-]+', text)\n",
        "\n",
        "    return {\n",
        "        \"Emails\": emails[0] if emails else \"Email Not Found\",\n",
        "        \"Phones\": phones[0] if phones else \"Phone Not Found\",\n",
        "        \"LinkedIn\": linkedin[0] if linkedin else \"Link Not Found\",\n",
        "        \"Github\": github[0] if github else \"Link Not Found\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZW0EY2NUDL1p"
      },
      "outputs": [],
      "source": [
        "# def clean_resume_text(text):\n",
        "#     text = text.lower()\n",
        "\n",
        "#     # Remove URLs, Emails and Phone numbers\n",
        "#     text = re.sub(r'\\S+@\\S+','', text)\n",
        "#     text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "#     # Remove special characters and numbers (alphanumeric and basic punctuation)\n",
        "#     text = re.sub(r'[^a-zA-Z\\s]', ' ', str(text))\n",
        "\n",
        "#     # Tokenize and remove stopwords\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     words = nltk.word_tokenize(text)\n",
        "#     filtered_text = [w for w in words if w not in stop_words]\n",
        "\n",
        "#     return \" \".join(filtered_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Wqq1YrrYk0Wx"
      },
      "outputs": [],
      "source": [
        "def extract_entities(text):\n",
        "    \"\"\"\n",
        "    Identify Names and Organizations using spaCy\n",
        "    \"\"\"\n",
        "\n",
        "    doc = nlp(text)\n",
        "    entities = {\n",
        "        \"Name\": [],\n",
        "        \"Organizations\": []\n",
        "    }\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            entities[\"Name\"].append(ent.text)\n",
        "        elif ent.label_ == \"ORG\":\n",
        "            entities[\"Organizations\"].append(ent.text)\n",
        "\n",
        "    primary_name = entities[\"Name\"][0] if entities[\"Name\"] else \"Not Identified\"\n",
        "\n",
        "    return {\n",
        "        \"Candidate Name\": primary_name,\n",
        "        \"All Names Found\": list(set(entities[\"Name\"])),\n",
        "        \"Comapanies/Institutions\": list(set(entities[\"Organizations\"]))\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dXUkLQqtoauZ"
      },
      "outputs": [],
      "source": [
        "def extract_skills(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    matcher = PhraseMatcher(nlp.vocab, attr = \"LOWER\")\n",
        "\n",
        "    # Skill categories\n",
        "    skills_db = {\n",
        "        \"Programming\": [\"Python\", \"Java\", \"C++\", \"JavaScript\", \"SQL\", \"GO\", \"Rust\"],\n",
        "        \"Machine Learning\": [\"PyTorch\", \"TensorFlow\", \"Scikit-learn\", \"NLP\", \"Computer Vision\"],\n",
        "        \"Cloud\": [\"AWS\", \"Azure\", \"Docker\", \"Kubernetes\", \"GCP\"],\n",
        "        \"Tools\": [\"Git\", \"Jira\", \"Excel\", \"Tableau\"]\n",
        "    }\n",
        "\n",
        "    # Add petterns to matcher\n",
        "    for category, skill_list in skills_db.items():\n",
        "        patterns = [nlp.make_doc(skill) for skill in skill_list]\n",
        "        matcher.add(category, patterns)\n",
        "\n",
        "    doc = nlp(text)\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    # Extraction of found skills\n",
        "    found_skills = {}\n",
        "    for match_id, start, end in matches:\n",
        "        category = nlp.vocab.strings[match_id]\n",
        "        skill = doc[start:end].text\n",
        "\n",
        "        if category not in found_skills:\n",
        "            found_skills[category] = set()\n",
        "\n",
        "        found_skills[category].add(skill)\n",
        "\n",
        "    # Conversion from sets to list for JSON output\n",
        "    return {k: list(v) for k, v in found_skills.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a1DLtvLztjDP"
      },
      "outputs": [],
      "source": [
        "def segment_resume(text):\n",
        "    # Standardized headers\n",
        "    sections = [\n",
        "        'EDUCATION',\n",
        "        'EXPERIENCE',\n",
        "        'WORK EXPERIENCE',\n",
        "        'PROJECTS',\n",
        "        'SKILLS',\n",
        "        'CERTIFICATIONS',\n",
        "        'SUMMARY'\n",
        "    ]\n",
        "\n",
        "    header_pattern = r'(?i)\\b(?:'+'|'.join(sections) + r')\\b'\n",
        "\n",
        "    # Find all matches and their positions\n",
        "    matches = list(re.finditer(header_pattern, text))\n",
        "\n",
        "    segmented_data = {}\n",
        "\n",
        "    if not matches:\n",
        "        return {\"Full text\":text}\n",
        "\n",
        "    # Iterate through matches to slice the text\n",
        "    for i in range(len(matches)):\n",
        "        start_idx = matches[i].start()\n",
        "        header_name = matches[i].group().upper()\n",
        "\n",
        "        # The end of this section is start of the next one\n",
        "        if i + 1 < len(matches):\n",
        "            end_idx = matches[i+1].start()\n",
        "        else:\n",
        "            end_idx = len(text)\n",
        "\n",
        "        content = text[start_idx:end_idx].replace(header_name, \"\").strip()\n",
        "        segmented_data[header_name] = content\n",
        "\n",
        "    return segmented_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4q9oLdEWxPNP"
      },
      "outputs": [],
      "source": [
        "def safe_resume_parser(pdf_file):\n",
        "    try:\n",
        "        # Extraction with validation\n",
        "        raw_text = extract_text_from_pdf(pdf_file)\n",
        "        if \"Error\" in raw_text:\n",
        "            return {\"Status\":\"Failed\", \"Reason\":\"Invalid PDF or Image_based PDF\"}\n",
        "\n",
        "        # Sequential Extraction\n",
        "        contacts = extract_contact_info(raw_text)\n",
        "        entities = extract_entities(raw_text)\n",
        "        segments = segment_resume(raw_text)\n",
        "        skills = extract_skills(raw_text)\n",
        "\n",
        "        # Handle missing data\n",
        "        candidate_name = entities.get(\"Candidate Name\", \"Not Identified\")\n",
        "        if candidate_name == \"Not Identified\" and contacts[\"Emails\"] != \"Not Found\":\n",
        "            candidate_name = contacts[\"Emails\"].split('@')[0].capitalize()\n",
        "\n",
        "        return {\n",
        "            \"Status\": \"Success\",\n",
        "            \"Metadata\": {\n",
        "                \"Filename\": pdf_file.name.split('/')[-1],\n",
        "                \"Text_Length\": len(raw_text)\n",
        "            },\n",
        "            \"Extracted_Data\": {\n",
        "                \"Name\": candidate_name,\n",
        "                \"Contact\": contacts,\n",
        "                \"Sections_Detected\": list(segments.keys()),\n",
        "                \"Skills\": skills\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"Status\": \"Critical Error\", \"Details\": str(e)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fPhZFj3vqhrN"
      },
      "outputs": [],
      "source": [
        "def calculate_match_score(resume_text, job_description):\n",
        "    \"\"\"\n",
        "    Similarity score between a resume and a JD\n",
        "    \"\"\"\n",
        "\n",
        "    text_list = [resume_text, job_description]\n",
        "\n",
        "    # Initialize Vectorizer\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    # Transform text into a matrix of numbers(vectors)\n",
        "    tfidf_matrix = vectorizer.fit_transform(text_list)\n",
        "\n",
        "    # Calculate similarity\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "\n",
        "    # Score as a percentage\n",
        "    match_score = round(similarity_matrix[0][0] * 100, 2)\n",
        "\n",
        "    return match_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MPK8ia-w9L6O"
      },
      "outputs": [],
      "source": [
        "def resume_parser(pdf_file, job_desc):\n",
        "    # Parse resume\n",
        "    parsed_result = safe_resume_parser(pdf_file)\n",
        "\n",
        "    if parsed_result[\"Status\"] != \"Success\":\n",
        "        return parsed_result, 0\n",
        "\n",
        "    # Get raw text and calculate score\n",
        "    raw_resume_text = extract_text_from_pdf(pdf_file)\n",
        "    score = calculate_match_score(raw_resume_text, job_desc)\n",
        "\n",
        "    return parsed_result[\"Extracted_Data\"], f\"{score} % Match\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mUP17LHs9R0p"
      },
      "outputs": [],
      "source": [
        "# New Gradio UI\n",
        "interface = gr.Interface(\n",
        "    fn = resume_parser,\n",
        "    inputs = [\n",
        "        gr.File(label = \"Upload Resume (PDF only)\"),\n",
        "        gr.Textbox(label = \"Paste Job Description (JD) here\", lines = 10)\n",
        "    ],\n",
        "    outputs = [\n",
        "        gr.JSON(label = \"Parsed Details\"),\n",
        "        gr.Label(label = \"ATS Match Score\")\n",
        "    ],\n",
        "    title = \"AI Resume Parser\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "R74lHqvO_n4W",
        "outputId": "529c59a6-4d1e-47ac-81e3-a16704a5126a"
      },
      "outputs": [],
      "source": [
        "interface.launch(debug = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
